{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11198ae0-6ca4-4b0d-b839-2200c779d226",
   "metadata": {},
   "source": [
    "## True Postive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c192335-5955-4429-ad46-aef5bdb5476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def search_pdb_with_json_query(json_query):\n",
    "    base_url = \"https://search.rcsb.org/rcsbsearch/v2/query\"\n",
    "\n",
    "    # URL-encode the JSON query and construct the API endpoint\n",
    "    encoded_query = json.dumps(json_query)\n",
    "    endpoint = f\"{base_url}?json={encoded_query}\"\n",
    "\n",
    "    try:\n",
    "        # Make the HTTP GET request to the API endpoint\n",
    "        response = requests.get(endpoint)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the response JSON\n",
    "        response_data = response.json()\n",
    "\n",
    "        # Extract and return the search results\n",
    "        return response_data[\"result_set\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error occurred during the API request:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2cc9dc0-6d69-4857-9b15-a18e721073b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of the TP from PDB:  8599\n"
     ]
    }
   ],
   "source": [
    "json_query = {\n",
    "  \"query\": {\n",
    "    \"type\": \"group\",\n",
    "    \"logical_operator\": \"and\",\n",
    "    \"nodes\": [\n",
    "      {\n",
    "        \"type\": \"group\",\n",
    "        \"nodes\": [\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"entity_poly.rcsb_sample_sequence_length\",\n",
    "              \"operator\": \"greater_or_equal\",\n",
    "              \"negation\": False,\n",
    "              \"value\": 20\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_entry_info.selected_polymer_entity_types\",\n",
    "              \"operator\": \"exact_match\",\n",
    "              \"negation\": False,\n",
    "              \"value\": \"Protein (only)\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_entry_info.polymer_entity_count_protein\",\n",
    "              \"operator\": \"equals\",\n",
    "              \"negation\": False,\n",
    "              \"value\": 2\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"logical_operator\": \"and\"\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"terminal\",\n",
    "        \"service\": \"text\",\n",
    "        \"parameters\": {\n",
    "          \"attribute\": \"rcsb_entry_info.deposited_polymer_entity_instance_count\",\n",
    "          \"operator\": \"equals\",\n",
    "          \"negation\": False,\n",
    "          \"value\": 2\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"terminal\",\n",
    "        \"service\": \"text\",\n",
    "        \"parameters\": {\n",
    "          \"attribute\": \"rcsb_assembly_info.polymer_entity_instance_count\",\n",
    "          \"operator\": \"equals\",\n",
    "          \"negation\": False,\n",
    "          \"value\": 2\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"group\",\n",
    "        \"nodes\": [\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_entry_info.structure_determination_methodology\",\n",
    "              \"value\": \"experimental\",\n",
    "              \"operator\": \"exact_match\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"terminal\",\n",
    "            \"service\": \"text\",\n",
    "            \"parameters\": {\n",
    "              \"attribute\": \"rcsb_entity_source_organism.ncbi_parent_scientific_name\",\n",
    "              \"value\": \"Eukaryota\",\n",
    "              \"operator\": \"exact_match\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"logical_operator\": \"and\"\n",
    "      }\n",
    "    ],\n",
    "    \"label\": \"text\"\n",
    "  },\n",
    "  \"return_type\": \"entry\",\n",
    "  \"request_options\": {\n",
    "    \"paginate\": {\n",
    "      \"start\": 0,\n",
    "      \"rows\": 10000\n",
    "    },\n",
    "    \"results_content_type\": [\n",
    "      \"experimental\"\n",
    "    ],\n",
    "    \"sort\": [\n",
    "      {\n",
    "        \"sort_by\": \"score\",\n",
    "        \"direction\": \"desc\"\n",
    "      }\n",
    "    ],\n",
    "    \"scoring_strategy\": \"combined\"\n",
    "  }\n",
    "}\n",
    "# Perform the search and get the results\n",
    "search_results = search_pdb_with_json_query(json_query)\n",
    "\n",
    "print(\"The number of the TP from PDB: \", len(search_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfa8c674-f8de-4c90-847e-ce20b2f16376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select pairs randomly from these set\n",
    "import random\n",
    "def random_select(search_results, sampling_num):\n",
    "    indexes = []\n",
    "    res = []\n",
    "    while len(indexes) < sampling_num:\n",
    "        index = random.randrange(0, len(search_results), 1) \n",
    "        if index not in indexes:\n",
    "            indexes.append(index)\n",
    "            res.append(search_results[index]['identifier'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a773a2b5-fae2-49bf-8dd8-73cab41f6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_fasta_from_pdb(entry_id):\n",
    "    \"\"\"Retrieve FASTA sequence from RCSB PDB using entry_id.\"\"\"\n",
    "    # Base URL for the RCSB PDB RESTful API for FASTA format\n",
    "    base_url = f\"https://www.rcsb.org/fasta/entry/{entry_id}\"\n",
    "    \n",
    "    # Send a GET request to the API to fetch the FASTA data\n",
    "    response = requests.get(base_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80d3a874-8b7e-4d91-9d3d-47fc8386aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fasta_input_and_pair(fasta_sequence: str, input_file: str, custom_pair: str):\n",
    "# I don't use the uniprot protein id in this case actually cause some of the entry id failed to match \n",
    "# the uniprot id. And almost all the them are not in exact form with the proteins listed in the uniprot\n",
    "# good for data augmentation in this case. and easier to track the exact aa seq\n",
    "    fastas = fasta_sequence.strip().split('\\n')\n",
    "    name = []\n",
    "    seq = []\n",
    "    for i in fastas:\n",
    "        items = i.strip().split(\"|\")\n",
    "        # print(items)\n",
    "        for item in items:\n",
    "            if item.strip()[0] == \">\":\n",
    "                name.append(item.strip()[1:])\n",
    "                break\n",
    "            if item.isupper():\n",
    "                seq.append(item.strip())\n",
    "                continue\n",
    "    # update \n",
    "    n = len(name)\n",
    "    for i in range(n):\n",
    "        if len(seq[i]) < 20:\n",
    "            return \n",
    "    with open(input_file, \"a\") as file:\n",
    "        for i in range(n):\n",
    "            file.write(f\">{name[i]}\\n\")\n",
    "            file.write(f\"{seq[i]}\\n\")\n",
    "    with open(custom_pair, \"a\") as file:\n",
    "        for i in range(n-1):\n",
    "            file.write(f\"{name[i]};\")\n",
    "        file.write(f\"{name[n-1]}\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56f102cf-33bd-472d-a698-1acfc5756a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_entries = []\n",
    "n = len(search_results)\n",
    "for i in range(n):\n",
    "    tp_entries.append(search_results[i]['identifier'])\n",
    "\n",
    "fasta_input = \"/n/home10/ytingliu/alphapulldown_new/8599_tp_input.fasta\"\n",
    "pair_input = \"/n/home10/ytingliu/alphapulldown_new/8599_tp_pairs.txt\"\n",
    "for entry_id in tp_entries:\n",
    "    fasta_sequence = get_fasta_from_pdb(entry_id)\n",
    "    update_fasta_input_and_pair(fasta_sequence, fasta_input, pair_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0e2bd-c939-4d38-b53a-865e81a3af5e",
   "metadata": {},
   "source": [
    "## True Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812a0ae0-c78a-41c0-87ab-5b071ae7be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. kinase with its protein substrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75af2cff-1bb5-4a34-bcd2-37f4287c4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. pairs for prokaryotes and eukaryotes\n",
    "\n",
    "# Bacteria (eubacteria) (taxonomy_id:2) ba review = True 336,064 results\n",
    "# Eukaryota (eucaryotes) (taxonomy_id:2759) review = True 197,016 results\n",
    "# Archaea (taxonomy_id:2157) review = True 19,716 results\n",
    "# Homo sapiens (species) (taxonomy_id:9606) review = True 20,429 results\n",
    "\n",
    "# remove based on sequence similarity? \n",
    "\n",
    "# since the pagnition nature of large uniprot query, the best method would be download all results and then use it as the base data\n",
    "# the random algorithm is low efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e367e82-7be5-4921-a275-710e64a8ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random pairs between archaea and homo sapien\n",
    "import random\n",
    "import os\n",
    "def random_pair(file1_path, file2_path, n):\n",
    "    # Lists to store protein names and sequences\n",
    "    protein_names_1 = []\n",
    "    protein_sequences_1 = []\n",
    "    protein_names_2 = []\n",
    "    protein_sequences_2 = []\n",
    "\n",
    "    with open(file1_path, \"r\") as file1:\n",
    "        lines = file1.readlines()\n",
    "        for i in range(0, len(lines), 2):\n",
    "            protein_names_1.append(lines[i].strip())\n",
    "            protein_sequences_1.append(lines[i + 1].strip())\n",
    "\n",
    "    with open(file2_path, \"r\") as file2:\n",
    "        lines = file2.readlines()\n",
    "        for i in range(0, len(lines), 2):\n",
    "            protein_names_2.append(lines[i].strip())\n",
    "            protein_sequences_2.append(lines[i + 1].strip())\n",
    "\n",
    "    selected_proteins_1 = random.sample(list(zip(protein_names_1, protein_sequences_1)), n)\n",
    "    selected_proteins_2 = random.sample(list(zip(protein_names_2, protein_sequences_2)), n)\n",
    "\n",
    "    protein_pairs = [(protein1, protein2) for protein1, protein2 in zip(selected_proteins_1, selected_proteins_2)]\n",
    "    return protein_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aad77b5a-1bcc-4699-877d-9b5010f1b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1_path = \"/n/home10/ytingliu/uniprot_database/uniprotkb_taxonomy_id_2157_AND_reviewed.txt\"\n",
    "file2_path = \"/n/home10/ytingliu/uniprot_database/uniprotkb_taxonomy_id_9606_AND_reviewed.txt\"\n",
    "fasta_input = \"/n/home10/ytingliu/alphapulldown_new/6000_tn_input.fasta\"\n",
    "pair_input = \"/n/home10/ytingliu/alphapulldown_new/6000_tn_pairs.txt\"\n",
    "res = random_pair(file1_path, file2_path, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d69c7fa1-dc7a-4d87-b47b-bde90c5f5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fasta_input, \"a\") as file:\n",
    "    for pair in res:\n",
    "        file.write(f\"{pair[0][0]}\\n\")\n",
    "        file.write(f\"{pair[0][1]}\\n\")\n",
    "        file.write(f\"{pair[1][0]}\\n\")\n",
    "        file.write(f\"{pair[1][1]}\\n\")\n",
    "with open(pair_input, \"a\") as file:\n",
    "    for pair in res:\n",
    "        file.write(f\"{pair[0][0][1:]};{pair[1][0][1:]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392c4a37-bd61-4e34-acde-664137578c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q8PSJ3_and_Q5T280\n"
     ]
    }
   ],
   "source": [
    "# reinburse the nagative dataset\n",
    "# add new combi from existing MSA (rather than from the fasta list)\n",
    "# extracting and split tn name\n",
    "def get_subdirectories(directory):\n",
    "    subdirectories = []\n",
    "    target_directories = []\n",
    "    for item in os.listdir(directory):\n",
    "        if \"6000_tn\" in item:\n",
    "            target_dir = os.path.join(directory, item)\n",
    "            for sub_item in os.listdir(target_dir):\n",
    "                subdir_item_path = os.path.join(target_dir, sub_item)\n",
    "                if os.path.isdir(subdir_item_path):\n",
    "                    subdirectories.append(sub_item)\n",
    "    return subdirectories\n",
    "\n",
    "# Specify the directory for which you want to extract subdirectories\n",
    "directory_path = \"/n/holyscratch01/ramanathan_lab/yuting/outputs\"\n",
    "\n",
    "subdirectories = get_subdirectories(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59de2c80-da73-4794-b977-b2777aeb2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fasta = set()\n",
    "for pair in subdirectories:\n",
    "    if '_and_' in pair:\n",
    "        valid_fasta.add(pair.split('_and_')[0])\n",
    "        valid_fasta.add(pair.split('_and_')[1])\n",
    "    \n",
    "file_path = \"valid_tn_msas.txt\"\n",
    "\n",
    "# Writing set elements to a text file\n",
    "with open(file_path, \"w\") as file:\n",
    "    for element in valid_fasta:\n",
    "        file.write(element + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a06b5271-85ff-48e7-880d-63944bd06164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 500 new pairs using the existing msa to enrich the dataset\n",
    "pair_input = \"/n/home10/ytingliu/alphapulldown_new/6000_tn_pairs.txt\"\n",
    "archaea = []\n",
    "human = []\n",
    "pairs = set()\n",
    "new_pairs = []\n",
    "with open(pair_input, 'r') as file:\n",
    "    for line in file:\n",
    "        res = line.strip().split(';')\n",
    "        archaea.append(res[0])\n",
    "        human.append(res[1])\n",
    "        pairs.add((res[0], res[1]))\n",
    "\n",
    "count = 0\n",
    "while count < 500:\n",
    "    selected_archaea = random.choice(archaea)\n",
    "    selected_human = random.choice(human)\n",
    "    if (selected_archaea, selected_human) not in pairs and selected_archaea in valid_fasta and selected_human in valid_fasta:\n",
    "        new_pairs.append([selected_archaea, selected_human])\n",
    "        count += 1\n",
    "with open(\"/n/home10/ytingliu/alphapulldown_new/500_tn_pairs.txt\", \"a\") as file:\n",
    "    for pair in new_pairs:\n",
    "        file.write(f\"{pair[0]};{pair[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99dd521-516d-426a-8663-ced5989c0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_input = \"/n/home10/ytingliu/alphapulldown_new/6000_tn_pairs.txt\"\n",
    "archaea = []\n",
    "human = []\n",
    "pairs = set()\n",
    "new_pairs = []\n",
    "with open(pair_input, 'r') as file:\n",
    "    for line in file:\n",
    "        res = line.strip().split(';')\n",
    "        archaea.append(res[0])\n",
    "        human.append(res[1])\n",
    "        pairs.add((res[0], res[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
